<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0,minimum-scale=1"><title>Research - Junhong Shen</title>
<meta property="og:title" content="Research - Junhong Shen">
<meta property="og:type" content="website">


<meta property="og:image" content="img/main2.jpg">

<meta property="og:url" content="https://sjunhongshen.github.io/research/">

<link rel="me" href="https://twitter.com/@JunhongShen1">
<meta name="twitter:card" content="summary">
<meta name="twitter:site" content="@JunhongShen1">
<meta name="twitter:creator" content="@JunhongShen1">




<link rel="stylesheet" href="https://sjunhongshen.github.io/css/style.min.css">

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/svg+xml" href="/favicon.svg">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<meta name="theme-color" content="#ffffff">
<link rel="webmention" href="https://webmention.io/hugocisneros.com/webmention">
<link rel="pingback" href="https://webmention.io/hugocisneros.com/xmlrpc">


<script>function updateMode(){localStorage.theme==="dark"||!("theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?document.documentElement.classList.add("dark"):document.documentElement.classList.remove("dark")}function toggleMode(){localStorage.theme==="dark"?localStorage.theme="light":localStorage.theme="dark",updateMode()}window.onload=updateMode();function toggleMenu(){let e=document.getElementById("navbar-default");e.classList.contains("hidden")?e.classList.remove("hidden"):e.classList.add("hidden")}</script>

  </head>
  <body>
    <header class="md:px-0 px-2">
        <nav >
  <div class="container flex flex-wrap justify-between items-center mx-auto">
    <div class="nav-main my-2.5">
      <a href="https://sjunhongshen.github.io/" class="nav-title py-2.5 text-2xl text-zinc-600 dark:text-zinc-300 hover:border-b-0">Junhong Shen</a>
    </div>
    <button type="button"
            onclick="toggleMenu()"
            class="inline-flex items-center p-2 ml-3
                  text-sm text-gray-500
                  rounded-lg md:hidden hover:bg-gray-100
                  focus:outline-none focus:ring-2
                  focus:ring-gray-200 dark:text-gray-400
                  dark:hover:bg-gray-700 dark:focus:ring-gray-600"
            aria-controls="navbar-default"
            aria-expanded="false">
        <span class="sr-only">Open main menu</span>
        <svg class="w-6 h-6" aria-hidden="true" fill="currentColor"
             viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg">
          <path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1
                           0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0
                           01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0
                           01-1-1z" clip-rule="evenodd"></path>
        </svg>
    </button>
    <div class="hidden w-full md:block md:w-auto" id="navbar-default">
      <ul class="grid md:grid-flow-col items-center justify-between text-lg my-2.5">
        
        <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
          <a class="text-zinc-600 dark:text-zinc-300
                    hover:border-b-0" href="/about/">About</a>
        </li>
        
        <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
          <a class="text-zinc-600 dark:text-zinc-300
                    hover:border-b-0" href="/research/">Research</a>
        </li>
        
        <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
          <a class="text-zinc-600 dark:text-zinc-300
                    hover:border-b-0" href="/posts/">Posts</a>
        </li>
        
        <li class="p-2.5 md:first:pl-0 md:border-none border-b list-none">
          <a class="text-zinc-600 dark:text-zinc-300
                    hover:border-b-0" href="/about/cv.pdf">CV</a>
        </li>
        
        <li class="h-7 pl-2.5 pr-0 list-none">
          <button type="button" onclick="toggleMode()" class="h-full"  aria-label="Toggle between dark and light mode">
            <img class="h-7 w-7 max-h-full mb-1.5 p-1.5 hidden dark:inline"
                 id="ligh-mode-button-img"
                 alt="A sun icon for switching to light mode"
                 src="https://sjunhongshen.github.io/img/light_mode.svg">
            <img class="h-7 w-7 max-h-full mb-1.5 p-1.5 inline dark:hidden"
                 id="dark-mode-button-img"
                 alt="A moon icon for switching to dark mode"
                 src="https://sjunhongshen.github.io/img/dark_mode.svg">
          </button>
        </li>
      </ul>
    </div>
  </div>
</nav>


    </header>
    <main class="content h-card container mt-2 m-auto
                 leading-loose md:px-0 px-2 z-0 Page(/research/_index.md)"
          role="main">
    <h1 id="resarch">Resarch</h1>
<p>I&rsquo;m interested in machine learning and deep learning in general. My current research focuses mainly on the practical side of ML, i.e., developing effective ML tools and pipelines for diverse applications in real life. I&rsquo;m particuarly intereted in enhancing LLM’s interaction with real-world applications by developing efficient and unified multi-modal models and building LLM agents capable of environment (e.g., browsers, command lines, IDEs) and user interactions. Besides, I also study:</p>
<ul>
<li>
<p>Automated machine learning (AutoML): how do we use neural architecture search (NAS) to generate effective and task-specific neural network architectures for different downstream problems?</p>
</li>
<li>
<p>Tranfer learning to scientific domains: how can we leverage existing large-scale pretrained models effectvely for solving problems that are not within the model&rsquo;s pretraining domain and modality?</p>
</li>
</ul>
<p> </p>
<h2 id="talks">Talks</h2>
<ul>
<li><a href="https://ai4sciencetalks.github.io/projects/orca_pde_junhong/"><em>Cross-Modal Fine-Tuning</em></a>, AI4Science Talks, March 20, 2023.</li>
<li><a href="https://automlpodcast.com/episode/dash-how-to-search-over-convolutions"><em>DASH: How to Search Over Convolutions</em></a>, The AutoML Podcast, December 19, 2022.</li>
<li><em>Tackling Diverse Tasks with Neural Architecture Search</em>, Deep Learning Machine Learning Journal Club, Mayo Clinic, October 17, 2022.</li>
</ul>
<p> </p>
<!-- raw HTML omitted -->
<!-- raw HTML omitted -->

    
        <h2 id="publications">Publications</h2>
    
    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="thinking-vs-doing-agents-that-reason-by-scaling-test-time-interaction"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction paper illustration"
             itemprop="url" src="img/tti.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen*</span></b>,
        
        <span itemprop="author">Hao Bai*, Lunjun Zhang, Yifei Zhou, Amrith Setlur, Shengbang Tong, Diego Caples, Nan Jiang, Tong Zhang, Ameet Talwalkar</span>,
        
        
        <span itemprop="author">Aviral Kumar</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2506.07976">
                <span itemprop="name">Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</span>
            </a>
        </b>
        <meta itemprop="headline" content="Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">Preprint</i></span>,
        
        <meta itemprop="datePublished" content="">.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">The current paradigm of test-time scaling relies on generating long reasoning traces (&#34;thinking&#34; more) before producing a response. In this work, we propose to scale test-time interaction, an untapped dimension of test-time scaling that increases the agent&#39;s interaction horizon to enable running rich behaviors such as exploration, backtracking, and dynamic re-planning within a single rollout.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2506.07976">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/test-time-interaction/TTI">code</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://test-time-interaction.github.io/">website</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://x.com/jackbai_jkb/status/1933097072028156127">blogpost</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-thinking-vs.-doing-agents-that-reason-by-scaling-test-time-interaction">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="codepde-benchmarking-llms-abilities-to-solve-pdes-through-code-generation"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="CodePDE: Benchmarking LLMs&#39; Abilities to Solve PDEs through Code Generation paper illustration"
             itemprop="url" src="img/codepde.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <span itemprop="author">Shanda Li, Tanya Marwah</span>,
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Weiwei Sun, Andrej Risteski, Yiming Yang</span>,
        
        
        <span itemprop="author">Ameet Talwalkar</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2505.08783">
                <span itemprop="name">CodePDE: Benchmarking LLMs&#39; Abilities to Solve PDEs through Code Generation</span>
            </a>
        </b>
        <meta itemprop="headline" content="CodePDE: Benchmarking LLMs&#39; Abilities to Solve PDEs through Code Generation"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">Preprint</i></span>,
        
        <meta itemprop="datePublished" content="">.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">We frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving---reasoning, debugging, self-refinement, and test-time scaling---all without task-specific tuning.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2505.08783">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/LithiumDA/CodePDE">code</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-codepde-benchmarking-llms-abilities-to-solve-pdes-through-code-generation">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="mixtureofmamba-enhancing-multimodal-statespace-models-with-modalityaware-sparsity"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Mixture‑of‑Mamba: Enhancing Multi‑Modal State‑Space Models with Modality‑Aware Sparsity paper illustration"
             itemprop="url" src="img/mamba.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <span itemprop="author">Weixin Liang*</span>,
        
        <b><span itemprop="author">Junhong Shen*</span></b>,
        
        <span itemprop="author">Genghan Zhang, Ning Dong, Luke Zettlemoyer</span>,
        
        
        <span itemprop="author">Lili Yu</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2501.16295">
                <span itemprop="name">Mixture‑of‑Mamba: Enhancing Multi‑Modal State‑Space Models with Modality‑Aware Sparsity</span>
            </a>
        </b>
        <meta itemprop="headline" content="Mixture‑of‑Mamba: Enhancing Multi‑Modal State‑Space Models with Modality‑Aware Sparsity"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">ICLR Scalable Optimization for Efficient and Adaptive Foundation Models Workshop</i></span>,
        
        <meta itemprop="datePublished" content="2025 (Oral, top 8/96)">2025 (Oral, top 8/96).
        </div><div itemprop="description" class="pub-description text-sm my-1.5">We propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block. Building on Mixture-of-Transformers, we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2501.16295">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/Weixin-Liang/Mixture-of-Mamba">code</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-mixtureofmamba-enhancing-multimodal-statespace-models-with-modalityaware-sparsity">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="cat-content-adaptive-image-tokenization"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="CAT: Content-Adaptive Image Tokenization paper illustration"
             itemprop="url" src="img/cat.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Kushal Tirumala, Michihiro Yasunaga, Ishan Misra, Luke Zettlemoyer, Lili Yu</span>,
        
        
        <span itemprop="author">Chunting Zhou</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2501.03120">
                <span itemprop="name">CAT: Content-Adaptive Image Tokenization</span>
            </a>
        </b>
        <meta itemprop="headline" content="CAT: Content-Adaptive Image Tokenization"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">Preprint</i></span>,
        
        <meta itemprop="datePublished" content="">.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">Most existing image tokenizers encode images into a fixed number of tokens or patches. We introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts representation capacity based on the image content and encodes simpler images into fewer tokens. We design a caption-based evaluation system that leverages LLMs to predict content complexity and determine the optimal compression ratio for a given image.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2501.03120">paper</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-cat-content-adaptive-image-tokenization">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="scribeagent-towards-specialized-web-agents-using-production-scale-workflow-data"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data paper illustration"
             itemprop="url" src="img/agent.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Atishay Jain, Zedian Xiao, Ishan Amlekar, Mouad Hadji, Aaron Podolny</span>,
        
        
        <span itemprop="author">Ameet Talwalkar</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2411.15004">
                <span itemprop="name">ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data</span>
            </a>
        </b>
        <meta itemprop="headline" content="ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">ICLR Foundation Models in the Wild Workshop</i></span>,
        
        <meta itemprop="datePublished" content="2025">2025.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">Most LLM-based web agents rely on prompting general-purpose, proprietary models like GPT-4. We explore an alternative approach that fine-tunes open-source LLMs using production-scale workflow data. This simple yet effective approach achieves SOTA direct generation performance on Mind2Web and improves the task success rate by 7.3% over the previous best text-only web agents on WebArena.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2411.15004">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/colonylabs/ScribeAgent">code</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-scribeagent-towards-specialized-web-agents-using-production-scale-workflow-data">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="specialized-foundation-models-struggle-to-beat-supervised-baselines"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Specialized Foundation Models Struggle to Beat Supervised Baselines paper illustration"
             itemprop="url" src="img/fm.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <span itemprop="author">Zongzhe Xu</span>,
        
        <span itemprop="author">Ritvik Gupta, Wenduo Cheng, Alexander Shen</span>,
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Ameet Talwalkar</span>,
        
        
        <span itemprop="author">Mikhail Khodak</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2411.02796">
                <span itemprop="name">Specialized Foundation Models Struggle to Beat Supervised Baselines</span>
            </a>
        </b>
        <meta itemprop="headline" content="Specialized Foundation Models Struggle to Beat Supervised Baselines"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">NeurIPS FM4Science Workshop</i></span>,
        
        <meta itemprop="datePublished" content="2024">2024.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">We look at three modalities--genomics, satellite imaging, and time series--with multiple recent FMs and compare them to a standard supervised learning workflow (model development, hyperparameter tuning, and training, all using only data from the target task). We find that it is consistently possible to train simple supervised models that match or even outperform the latest foundation models.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2411.02796">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/ritvikgupta199/DASHA">code</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-specialized-foundation-models-struggle-to-beat-supervised-baselines">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="ups-efficiently-building-foundation-models-for-pde-solving-via-cross-modal-adaptation"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation paper illustration"
             itemprop="url" src="img/pde.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Tanya Marwah</span>,
        
        
        <span itemprop="author">Ameet Talwalkar</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2403.07187">
                <span itemprop="name">UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation</span>
            </a>
        </b>
        <meta itemprop="headline" content="UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">TMLR 2024 &amp; ICML AI4Science Workshop</i></span>,
        
        <meta itemprop="datePublished" content="2024 (Spotlight)">2024 (Spotlight).
        </div><div itemprop="description" class="pub-description text-sm my-1.5">UPS is developed for solving diverse spatiotemporal PDEs defined over various domains, dimensions, and resolutions. It unifies different PDEs into a consistent representation space and processes diverse collections of PDE data using a unified network architecture that combines LLMs with domain-specific neural operators.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2403.07187">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/sjunhongshen/UnifiedPDESolvers">code</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-ups-efficiently-building-foundation-models-for-pde-solving-via-cross-modal-adaptation">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="tag-llm-repurposing-general-purpose-llms-for-specialized-domains"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains paper illustration"
             itemprop="url" src="img/tag.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Neil Tenenholtz, James Brian Hall, David Alvarez-Melis</span>,
        
        
        <span itemprop="author">Nicolo Fusi</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2402.05140">
                <span itemprop="name">Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains</span>
            </a>
        </b>
        <meta itemprop="headline" content="Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">ICML</i></span>,
        
        <meta itemprop="datePublished" content="2024">2024.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">LLMs demonstrate proficiency in understanding natural language. However, their capabilities wane in highly specialized domains underrepresented in the pretraining corpus, such as physical and biomedical sciences. This work explores how to repurpose general LLMs into specialized task solvers through learning custom input tags to condition the LLM.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2402.05140">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/sjunhongshen/Tag-LLM">code</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-tag-llm-repurposing-general-purpose-llms-for-specialized-domains">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="cross-modal-fine-tuning-align-then-refine"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Cross-Modal Fine-Tuning: Align then Refine paper illustration"
             itemprop="url" src="img/orca.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Liam Li, Lucio Dery, Corey Staten, Mikhail Khodak, Graham Neubig</span>,
        
        
        <span itemprop="author">Ameet Talwalkar</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2302.05738">
                <span itemprop="name">Cross-Modal Fine-Tuning: Align then Refine</span>
            </a>
        </b>
        <meta itemprop="headline" content="Cross-Modal Fine-Tuning: Align then Refine"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">ICML</i></span>,
        
        <meta itemprop="datePublished" content="2023 ([Oral](https://icml.cc/virtual/2023/oral/25514))">2023 ([Oral](https://icml.cc/virtual/2023/oral/25514)).
        </div><div itemprop="description" class="pub-description text-sm my-1.5">ORCA is a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. It adapts to a target task via an align-then-refine workflow. Given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2302.05738">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/sjunhongshen/ORCA">code</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://icml.cc/virtual/2023/oral/25514">talk</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://twitter.com/atalwalkar/status/1625595956598022166">tweet</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-cross-modal-fine-tuning-align-then-refine">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="efficient-architecture-search-for-diverse-tasks"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Efficient Architecture Search for Diverse Tasks paper illustration"
             itemprop="url" src="img/dash.gif"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen*</span></b>,
        
        <span itemprop="author">Mikhail Khodak*</span>,
        
        
        <span itemprop="author">Ameet Talwalkar</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2204.07554">
                <span itemprop="name">Efficient Architecture Search for Diverse Tasks</span>
            </a>
        </b>
        <meta itemprop="headline" content="Efficient Architecture Search for Diverse Tasks"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">NeurIPS</i></span>,
        
        <meta itemprop="datePublished" content="2022">2022.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">DASH is developed for efficiently solving diverse ML problems outside of the well-researched domains such as vision and natural language processing. Being fast, simple, and broadly applicable, DASH fixes a standard CNN topology and searches for the right kernel sizes and dilation rates that its operations should take on. It expands the network capacity to extract features at multiple resolutions for different types of data while only requiring searching over the operation space.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2204.07554">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/sjunhongshen/DASH">code</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://blog.ml.cmu.edu/2022/10/14/tackling-diverse-tasks-with-neural-architecture-search/">blogpost</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-efficient-architecture-search-for-diverse-tasks">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="nas-bench-360-benchmarking-neural-architecture-search-on-diverse-tasks"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks paper illustration"
             itemprop="url" src="img/nb360.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <span itemprop="author">Renbo Tu*</span>,
        
        <span itemprop="author">Nicholas Roberts*</span>,
        
        <span itemprop="author">Mikhail Khodak</span>,
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Frederic Sala</span>,
        
        
        <span itemprop="author">Ameet Talwalkar</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2110.05668">
                <span itemprop="name">NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks</span>
            </a>
        </b>
        <meta itemprop="headline" content="NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">NeurIPS Datasets and Benchmarks Track</i></span>,
        
        <meta itemprop="datePublished" content="2022">2022.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">Neural architecture search (NAS) benchmarks and methods prioritize performance on well-studied tasks, e.g., image classification on CIFAR and ImageNet. To mitigate this bias, NAS-Bench-360 is a benchmark suite for evaluating state-of-the-art NAS methods on a diverse set of tasks. The selection spans different application domains, dataset sizes, problem dimensionalities, and learning objectives.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2110.05668">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/rtu715/NAS-Bench-360">code</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://nb360.ml.cmu.edu/">website</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://blog.ml.cmu.edu/2022/07/07/automl-for-diverse-tasks/">blogpost</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-nas-bench-360-benchmarking-neural-architecture-search-on-diverse-tasks">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="iterative-teacher-aware-learning"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Iterative Teacher-Aware Learning paper illustration"
             itemprop="url" src="img/ital.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <span itemprop="author">Luyao Yuan</span>,
        
        <span itemprop="author">Dongruo Zhou</span>,
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Jingdong Gao, Jeffrey L. Chen, Quanquan Gu, Ying Nian Wu</span>,
        
        
        <span itemprop="author">Song-Chun Zhu</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2110.00137">
                <span itemprop="name">Iterative Teacher-Aware Learning</span>
            </a>
        </b>
        <meta itemprop="headline" content="Iterative Teacher-Aware Learning"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">NeurIPS</i></span>,
        
        <meta itemprop="datePublished" content="2021">2021.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">In this paper, we propose a gradient optimization based teacher-aware learner who can incorporate teacher’s cooperative intention into the likelihood function and learn provably faster compared with the naive learning algorithms used in previous machine teaching works.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2110.00137">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/yuanluya/ITAL">code</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-iterative-teacher-aware-learning">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="theoretically-principled-deep-rl-acceleration-via-nearest-neighbor-function-approximation"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Theoretically Principled Deep RL Acceleration via Nearest Neighbor Function Approximation paper illustration"
             itemprop="url" src="img/nnac.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        
        <span itemprop="author">Lin F. Yang</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2110.04422">
                <span itemprop="name">Theoretically Principled Deep RL Acceleration via Nearest Neighbor Function Approximation</span>
            </a>
        </b>
        <meta itemprop="headline" content="Theoretically Principled Deep RL Acceleration via Nearest Neighbor Function Approximation"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">AAAI</i></span>,
        
        <meta itemprop="datePublished" content="2021">2021.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">We propose a theoretically principled nearest neighbor (NN) function approximator that can replace the value networks in deep RL methods. Inspired by human similarity judgments, the NN approximator estimates the action values using rollouts on past observations and can provably obtain a small regret bound that depends only on the intrinsic complexity of the environment.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2110.04422">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/sjunhongshen/NNAC">code</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://sjunhongshen.github.io/files/5535_Presentation.pdf">slides</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-theoretically-principled-deep-rl-acceleration-via-nearest-neighbor-function-approximation">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="mathematical-reconstruction-of-patient-specific-vascular-networks-based-on-clinical-images-and-global-optimization"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Mathematical Reconstruction of Patient-Specific Vascular Networks Based on Clinical Images and Global Optimization paper illustration"
             itemprop="url" src="img/network.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        <span itemprop="author">Abdul Hannan Faruqi, Yifan Jiang</span>,
        
        
        <span itemprop="author">Nima Maftoon</span>
        
        <br>
        <b>
            <a href="https://ieeexplore.ieee.org/abstract/document/9328247">
                <span itemprop="name">Mathematical Reconstruction of Patient-Specific Vascular Networks Based on Clinical Images and Global Optimization</span>
            </a>
        </b>
        <meta itemprop="headline" content="Mathematical Reconstruction of Patient-Specific Vascular Networks Based on Clinical Images and Global Optimization"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">IEEE Access</i></span>,
        
        <meta itemprop="datePublished" content="2021">2021.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">We developed a computational framework that takes 3D medical images as input and reconstructs complete, patient-specific vascular network models using a mathematical optimization procedure. Our framework extracts major vessels from the images and uses the organ geometry to select vessel termination points. Then, it generates the remainder network based on physiological optimality principles.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://ieeexplore.ieee.org/abstract/document/9328247">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://github.com/nmaftoon/VesselGen">code</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://drive.google.com/file/d/1avXWJDVLXO-B7Nklqduo317BcXtGq3HR/view">slides</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-mathematical-reconstruction-of-patient-specific-vascular-networks-based-on-clinical-images-and-global-optimization">cite</button></span></div>
    </div>
</div>

    
        <div class="pub-wrapper w-full inline-flex flex-wrap items-center
            my-1.5 p-2.5 rounded bg-slate-100 dark:bg-slate-700 box-border
            leading-tight justify-around" id="emergence-of-pragmatics-from-referential-game-between-theory-of-mind-agents"
     itemscope itemtype="http://schema.org/ScholarlyArticle">
    <div class="pub-image basis-40 grow-0 shrink" itemprop="image"
         itemscope itemtype="http://schema.org/ImageObject">
        <img alt="Emergence of Pragmatics from Referential Game between Theory of Mind Agents paper illustration"
             itemprop="url" src="img/comm.png"
             class="w-full"/>
    </div>
    <div class="publication grow shrink-2 basis-3/4 my-1.5 mx-2">
        
        <span itemprop="author">Luyao Yuan</span>,
        
        <span itemprop="author">Zipeng Fu, Jingyue Shen, Lu Xu</span>,
        
        <b><span itemprop="author">Junhong Shen</span></b>,
        
        
        <span itemprop="author">Song-Chun Zhu</span>
        
        <br>
        <b>
            <a href="https://arxiv.org/abs/2001.07752">
                <span itemprop="name">Emergence of Pragmatics from Referential Game between Theory of Mind Agents</span>
            </a>
        </b>
        <meta itemprop="headline" content="Emergence of Pragmatics from Referential Game between Theory of Mind Agents"/>
        <br>
        
        <div>
        In <span itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
            <i itemprop="name">Emergent Communication Workshop, NeurIPS</i></span>,
        
        <meta itemprop="datePublished" content="2019">2019.
        </div><div itemprop="description" class="pub-description text-sm my-1.5">We integrate the theory of mind (ToM) in a cooperative multi-agent pedagogical situation and propose an adaptive reinforcement learning (RL) algorithm to develop a communication protocol.</div><div class="pub-link-wrapper mt-1.5 text-sm"><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://arxiv.org/abs/2001.07752">paper</a></span><span class="publication-link bg-sky-600
                         dark:bg-sky-700 rounded-sm p-1
                         dark:hover:bg-sky-500
                         hover:bg-sky-800 mr-1"><a class="border-none
                         text-white
                         dark:text-zinc-200
                         hover:text-gray-900
                          hover:dark:text-gray-800"
                   itemprop="mainEntityOfPage" href="https://yuanluya.github.io/projects/RG/">website</a></span><span class="publication-link cursor-pointer
                         bg-sky-600 dark:bg-sky-700
                         hover:bg-sky-800
                         dark:hover:bg-sky-500
                         text-white
                         dark:text-zinc-200
                         dark:hover:text-gray-800
                         hover:text-gray-900
                         rounded-sm p-1 mr-1"><button class="open-modal text-inherit"
                        data-target="#open-modal-emergence-of-pragmatics-from-referential-game-between-theory-of-mind-agents">cite</button></span></div>
    </div>
</div>

    
    
    
<div id="#open-modal-thinking-vs.-doing-agents-that-reason-by-scaling-test-time-interaction"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-thinking-vs.-doing-agents-that-reason-by-scaling-test-time-interaction" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-e34a8b77">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="e34a8b77">@misc{shenbai2025tti,
 title={Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction}, 
 author={Junhong Shen and Hao Bai and Lunjun Zhang and Yifei Zhou and Amrith Setlur and Shengbang Tong and Diego Caples and Nan Jiang and Tong Zhang and Ameet Talwalkar and Aviral Kumar},
 year={2025},
 eprint={2506.07976},
 archivePrefix={arXiv},
 primaryClass={cs.LG},
 url={https://arxiv.org/abs/2506.07976},
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-codepde-benchmarking-llms-abilities-to-solve-pdes-through-code-generation"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-codepde-benchmarking-llms-abilities-to-solve-pdes-through-code-generation" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>CodePDE: Benchmarking LLMs&#39; Abilities to Solve PDEs through Code Generation</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-7878753b">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="7878753b">@misc{li2025codepde,
 title={CodePDE: An Inference Framework for LLM-driven PDE Solver Generation},
 author={Shanda Li and Tanya Marwah and Junhong Shen and Weiwei Sun and Andrej Risteski and Yiming Yang and Ameet Talwalkar},
 year={2025},
 eprint={2505.08783},
 archivePrefix={arXiv},
 primaryClass={cs.LG},
 url={https://arxiv.org/abs/2505.08783},
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-mixtureofmamba-enhancing-multimodal-statespace-models-with-modalityaware-sparsity"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-mixtureofmamba-enhancing-multimodal-statespace-models-with-modalityaware-sparsity" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Mixture‑of‑Mamba: Enhancing Multi‑Modal State‑Space Models with Modality‑Aware Sparsity</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-f2b42d8f">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="f2b42d8f">@misc{liangshen2025mixtureofmamba,
 title={Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity},
 author={Weixin Liang and Junhong Shen and Genghan Zhang and Ning Dong and Luke Zettlemoyer and Lili Yu},
 year={2025},
 eprint={2501.16295},
 archivePrefix={arXiv},
 primaryClass={cs.LG},
 url={https://arxiv.org/abs/2501.16295},
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-cat-content-adaptive-image-tokenization"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-cat-content-adaptive-image-tokenization" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>CAT: Content-Adaptive Image Tokenization</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-ac99cffe">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="ac99cffe">@misc{shen2024adaptivetokenizer,
 title={CAT: Content-Adaptive Image Tokenization},
 author={Junhong Shen and Kushal Tirumala and Michihiro Yasunaga and Ishan Misra and Luke Zettlemoyer and Lili Yu and Chunting Zhou},
 year={2025},
 eprint={2501.03120},
 archivePrefix={arXiv},
 primaryClass={cs.CV},
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-scribeagent-towards-specialized-web-agents-using-production-scale-workflow-data"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-scribeagent-towards-specialized-web-agents-using-production-scale-workflow-data" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-7cae9506">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="7cae9506">@misc{shen2024scribeagent,
 title={ScribeAgent: Towards Specialized Web Agents Using Production-Scale Workflow Data},
 author={Junhong Shen and Atishay Jain and Zedian Xiao and Ishan Amlekar and Mouad Hadji and Aaron Podolny and Ameet Talwalkar},
 year={2024},
 eprint={2411.15004},
 archivePrefix={arXiv},
 primaryClass={cs.CL},
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-specialized-foundation-models-struggle-to-beat-supervised-baselines"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-specialized-foundation-models-struggle-to-beat-supervised-baselines" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Specialized Foundation Models Struggle to Beat Supervised Baselines</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-df739554">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="df739554">@misc{xu2024specializedfm,
 title={Specialized Foundation Models Struggle to Beat Supervised Baselines},
 author={Zongzhe Xu and Ritvik Gupta and Wenduo Cheng and Alexander Shen and Junhong Shen and Ameet Talwalkar and Mikhail Khodak},
 year={2024},
 eprint={2411.02796},
 archivePrefix={arXiv},
 primaryClass={cs.LG},
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-ups-efficiently-building-foundation-models-for-pde-solving-via-cross-modal-adaptation"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-ups-efficiently-building-foundation-models-for-pde-solving-via-cross-modal-adaptation" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-b9f9ec24">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="b9f9ec24">@misc{shen2024ups, title={UPS: Efficiently Building Foundation Models for PDE Solving via Cross-Modal Adaptation},
 author={Junhong Shen and Tanya Marwah and Ameet Talwalkar},
 year={2024},
 eprint={2403.07187},
 archivePrefix={arXiv},
 primaryClass={cs.LG}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-tag-llm-repurposing-general-purpose-llms-for-specialized-domains"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-tag-llm-repurposing-general-purpose-llms-for-specialized-domains" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-74253170">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="74253170">@misc{shen2024tagllm,
 title={Tag-LLM: Repurposing General-Purpose LLMs for Specialized Domains}, 
 author={Junhong Shen and Neil Tenenholtz and James Brian Hall and David Alvarez-Melis and Nicolo Fusi},
 year={2024},
 eprint={2402.05140},
 archivePrefix={arXiv},
 primaryClass={cs.LG}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-cross-modal-fine-tuning-align-then-refine"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-cross-modal-fine-tuning-align-then-refine" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Cross-Modal Fine-Tuning: Align then Refine</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-3e5a543d">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="3e5a543d">@misc{shen2023orca,
 author = {Shen, Junhong and Li, Liam and Dery, Lucio M. and Staten, Corey and Khodak, Mikhail and Neubig, Graham and Talwalkar, Ameet},
 title = {Cross-Modal Fine-Tuning: Align then Refine},
 publisher = {ICML},
 year = {2023},
 url = {https://arxiv.org/abs/2302.05738}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-efficient-architecture-search-for-diverse-tasks"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-efficient-architecture-search-for-diverse-tasks" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Efficient Architecture Search for Diverse Tasks</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-a3c9498c">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="a3c9498c">@inproceedings{shen2022efficient,
 title={Efficient Architecture Search for Diverse Tasks},
 author={Shen, Junhong and Khodak, Mikhail and Talwalkar, Ameet},
 booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
 year={2022}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-nas-bench-360-benchmarking-neural-architecture-search-on-diverse-tasks"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-nas-bench-360-benchmarking-neural-architecture-search-on-diverse-tasks" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-ffebf1ae">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="ffebf1ae">@inproceedings{nasbench360,
 title={NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks},
 author={Renbo Tu and Nicholas Roberts and Mikhail Khodak and Junhong Shen and Frederic Sala and Ameet Talwalkar},
 booktitle={Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track},
 year={2022}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-iterative-teacher-aware-learning"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-iterative-teacher-aware-learning" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Iterative Teacher-Aware Learning</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-16c6bc8e">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="16c6bc8e">@inproceedings{yuan2021iterative,
 title={Iterative Teacher-Aware Learning},
 author={Luyao Yuan and Dongruo Zhou and Junhong Shen and Jingdong Gao and Jeffrey L. Chen and Quanquan Gu and Ying Nian Wu and Song-Chun Zhu},
 booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
 year={2021}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-theoretically-principled-deep-rl-acceleration-via-nearest-neighbor-function-approximation"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-theoretically-principled-deep-rl-acceleration-via-nearest-neighbor-function-approximation" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Theoretically Principled Deep RL Acceleration via Nearest Neighbor Function Approximation</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-1df0702f">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="1df0702f">@inproceedings{Shen2021TheoreticallyPD,
 title={Theoretically Principled Deep RL Acceleration via Nearest Neighbor Function Approximation},
 author={Junhong Shen and Lin F. Yang},
 booktitle={AAAI},
 year={2021}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-mathematical-reconstruction-of-patient-specific-vascular-networks-based-on-clinical-images-and-global-optimization"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-mathematical-reconstruction-of-patient-specific-vascular-networks-based-on-clinical-images-and-global-optimization" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Mathematical Reconstruction of Patient-Specific Vascular Networks Based on Clinical Images and Global Optimization</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-41c83acd">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="41c83acd">@article{shen2021reconstruction,
 author={Shen, Junhong and Faruqi, Abdul Hannan and Jiang, Yifan and Maftoon, Nima},
 journal={IEEE Access}, 
 title={Mathematical Reconstruction of Patient-Specific Vascular Networks Based on Clinical Images and Global Optimization}, 
 year={2021},
 volume={9},
 pages={20648-20661}
 }</code></pre>
    </div>
</div>


    
    
<div id="#open-modal-emergence-of-pragmatics-from-referential-game-between-theory-of-mind-agents"
     class="modal-window bg-tp-black invisible fixed top-0 left-0
            right-0 bottom-0 transition-all pointer-events-none z-50">
    <div class="absolute top-1/2 left-1/2 p-8 bg-white
                dark:bg-neutral-700

                max-w-full
                rounded-md max-w-4/5 -translate-x-1/2 -translate-y-1/2">
        <button id="#close-modal-emergence-of-pragmatics-from-referential-game-between-theory-of-mind-agents" title="Close" class="modal-close rounded-sm p-2">✕</button>
        <h3>Cite <i>Emergence of Pragmatics from Referential Game between Theory of Mind Agents</i></h3>
        <button class="modal-copy-citation pr-2 pl-1 my-2 ml-2
                       bg-sky-600 dark:bg-sky-700 rounded-sm
                       text-white
                       dark:text-zinc-200
                       dark:hover:text-gray-800
                       hover:text-gray-900
                       hover:bg-sky-800
                       disabled:bg-neutral-500
                       hover:disabled:text-white
                       dark:hover:disabled:text-zinc-200
                       dark:disabled:bg-neutral-500
                       dark:hover:bg-sky-500" id="#copy-06762a4a">Copy to clipboard</button>
        <pre class="bg-slate-100 dark:bg-slate-800"><code id="06762a4a">@article{Yuan2020EmergenceOP,
 title={Emergence of Pragmatics from Referential Game between Theory of Mind Agents},
 author={Luyao Yuan and Zipeng Fu and Jingyue Shen and Lu Xu and Junhong Shen and Song-Chun Zhu},
 journal={NeurIPS 2019 Workshop on Emergent Communication},
 year={2019}
 }</code></pre>
    </div>
</div>


    
    </main>
    <footer class="footer container h-10 text-center mt-1">
<hr class="my-4">
  <ul class="pl-0 mt-1">
    
    <li class="ml-2 first:before:content-none before:content-['•']
               inline-block list-none">
      <a class="ml-2 text-neutral-800
                dark:text-neutral-400 border-none"
          href="https://github.com/hugcis/hugo-astatine-theme">Code</a>
    </li>
    <li class="ml-2 first:before:content-none before:content-['•']
                text-neutral-800 dark:text-neutral-400 inline-block list-none">
      <span class="ml-2">© Junhong Shen 2025</span>
    </li>
  </ul>
</footer>


  </body>
</html>
